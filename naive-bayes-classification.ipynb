{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1539868,"sourceType":"datasetVersion","datasetId":908128}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Naive Bayes Algorithm Overview\nNaive Bayes classifiers are based on Bayes' theorem and make the assumption that the features are conditionally independent given the class. There are several types of Naive Bayes classifiers based on the nature of the data:\n\n- **Gaussian Naive Bayes:** For continuous data, assumes that the features follow a normal distribution.\n- **Multinomial Naive Bayes:** Suitable for discrete data, often used for text classification where feature vectors represent term frequencies.\n- **Bernoulli Naive Bayes:** For binary/boolean data where features are either present or absent.\n\n# Implementation Steps:\n\nFor most machine learning workflows involving Naive Bayes, these steps are followed:\n\n### 1. Data Preprocessing:\n\n- Split data into features (X) and target labels (y).\n- Handle missing data and convert categorical variables to numerical values if required.\n- Split the data into training and test sets.\n\n### 2. Model Training:\n\n- Select the appropriate Naive Bayes algorithm (Gaussian, Multinomial, or Bernoulli).\n- Train the model on the training set using fit().\n\n### 3. Prediction\n\n- Use the trained model to predict the target labels for the test set using predict().\n\n### 4. Evaluation:\n\n- Evaluate the model's performance using various metrics.\n\n# Evaluation Techniques:\nHere are some common evaluation metrics used for Naive Bayes:\n\n- **Confusion Matrix:** A table that shows the counts of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). It is used to calculate other metrics.\n\n- **Accuracy:** Measures the overall correctness of the model.\n- **Precision:** Measures how many of the predicted positive samples are actually positive.\n- **Recall (Sensitivity):** Measures how many of the actual positive samples are correctly predicted.\n- **F1-Score:** The harmonic mean of precision and recall, used when there is an uneven class distribution.\n- **ROC Curve and AUC:** Receiver Operating Characteristic curve plots the true positive rate against the false positive rate. The area under the curve (AUC) is a metric that summarizes the performance of the model.\n- **Cross-validation:** Perform k-fold cross-validation to ensure that the model generalizes well across different subsets of the data.\n\n","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-25T04:49:46.322706Z","iopub.execute_input":"2024-10-25T04:49:46.323264Z","iopub.status.idle":"2024-10-25T04:49:47.767032Z","shell.execute_reply.started":"2024-10-25T04:49:46.323208Z","shell.execute_reply":"2024-10-25T04:49:47.765607Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Importing the dataset","metadata":{}},{"cell_type":"code","source":"genclass = pd.read_csv(\"/kaggle/input/gender-classification-dataset/gender_classification_v7.csv\")\ngenclass","metadata":{"execution":{"iopub.status.busy":"2024-10-25T04:51:03.627496Z","iopub.execute_input":"2024-10-25T04:51:03.62819Z","iopub.status.idle":"2024-10-25T04:51:03.693007Z","shell.execute_reply.started":"2024-10-25T04:51:03.628141Z","shell.execute_reply":"2024-10-25T04:51:03.691699Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Info","metadata":{}},{"cell_type":"code","source":"genclass.info()","metadata":{"execution":{"iopub.status.busy":"2024-10-25T04:51:18.145607Z","iopub.execute_input":"2024-10-25T04:51:18.14619Z","iopub.status.idle":"2024-10-25T04:51:18.176512Z","shell.execute_reply.started":"2024-10-25T04:51:18.146134Z","shell.execute_reply":"2024-10-25T04:51:18.175044Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Finding Missing Valuess and Clearing","metadata":{}},{"cell_type":"code","source":"genclass.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-10-25T04:51:35.995166Z","iopub.execute_input":"2024-10-25T04:51:35.99577Z","iopub.status.idle":"2024-10-25T04:51:36.009148Z","shell.execute_reply.started":"2024-10-25T04:51:35.995717Z","shell.execute_reply":"2024-10-25T04:51:36.007427Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Describe","metadata":{}},{"cell_type":"code","source":"genclass.describe()","metadata":{"execution":{"iopub.status.busy":"2024-10-25T04:51:44.911659Z","iopub.execute_input":"2024-10-25T04:51:44.912287Z","iopub.status.idle":"2024-10-25T04:51:44.959112Z","shell.execute_reply.started":"2024-10-25T04:51:44.912227Z","shell.execute_reply":"2024-10-25T04:51:44.957087Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Label Encoding ","metadata":{}},{"cell_type":"code","source":"# cata to numerical\nfrom sklearn.preprocessing import LabelEncoder\n\n# Initialize the LabelEncoder\nlabel_encoder = LabelEncoder()\n\n# Apply label encoding to the 'City' column\ngenclass['gender'] = label_encoder.fit_transform(genclass['gender'])\n\n# Show the encoded data\ngenclass.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-25T04:52:14.333015Z","iopub.execute_input":"2024-10-25T04:52:14.333606Z","iopub.status.idle":"2024-10-25T04:52:14.974215Z","shell.execute_reply.started":"2024-10-25T04:52:14.333553Z","shell.execute_reply":"2024-10-25T04:52:14.972892Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"genclass.info()","metadata":{"execution":{"iopub.status.busy":"2024-10-25T04:52:23.063896Z","iopub.execute_input":"2024-10-25T04:52:23.064662Z","iopub.status.idle":"2024-10-25T04:52:23.080901Z","shell.execute_reply.started":"2024-10-25T04:52:23.064607Z","shell.execute_reply":"2024-10-25T04:52:23.079345Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Creating Target variable and features\n- X = Features \n- y = Target","metadata":{}},{"cell_type":"code","source":"# Splitting the data into features (X) and target (y)\nX = genclass.drop(['gender'], axis=1)\ny = genclass['gender']","metadata":{"execution":{"iopub.status.busy":"2024-10-25T04:53:02.367438Z","iopub.execute_input":"2024-10-25T04:53:02.368439Z","iopub.status.idle":"2024-10-25T04:53:02.376034Z","shell.execute_reply.started":"2024-10-25T04:53:02.368382Z","shell.execute_reply":"2024-10-25T04:53:02.374404Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train test Split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Splitting the dataset into train and test sets (80% train, 20% test)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T04:54:13.281644Z","iopub.execute_input":"2024-10-25T04:54:13.282886Z","iopub.status.idle":"2024-10-25T04:54:13.419339Z","shell.execute_reply.started":"2024-10-25T04:54:13.282819Z","shell.execute_reply":"2024-10-25T04:54:13.418104Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Gaussian Naive Bayes\n- No changes are needed if your features are continuous.\n- so we use the train data itself","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\n# 1. Gaussian Naive Bayes\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)\n\n# Make predictions with GaussianNB\ny_pred_gnb = gnb.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T04:55:17.668899Z","iopub.execute_input":"2024-10-25T04:55:17.669507Z","iopub.status.idle":"2024-10-25T04:55:17.687895Z","shell.execute_reply.started":"2024-10-25T04:55:17.669451Z","shell.execute_reply":"2024-10-25T04:55:17.686329Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Scaling","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\n# Scaling features for MultinomialNB (converts data to a range between 0 and 1)\nscaler = MinMaxScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n\n# y_train_scaled = scaler.fit_transform(y_train)\n# y_test_scaled = scaler.transform(y_test)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T04:56:01.543123Z","iopub.execute_input":"2024-10-25T04:56:01.544212Z","iopub.status.idle":"2024-10-25T04:56:01.559125Z","shell.execute_reply.started":"2024-10-25T04:56:01.544144Z","shell.execute_reply":"2024-10-25T04:56:01.557771Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Multinomial Naive Bayes","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\n# 2. Multinomial Naive Bayes\nmnb = MultinomialNB()\nmnb.fit(X_train_scaled, y_train)\n\n# Make predictions with MultinomialNB\ny_pred_mnb = mnb.predict(X_test_scaled)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T04:56:49.536433Z","iopub.execute_input":"2024-10-25T04:56:49.537405Z","iopub.status.idle":"2024-10-25T04:56:49.554393Z","shell.execute_reply.started":"2024-10-25T04:56:49.537346Z","shell.execute_reply":"2024-10-25T04:56:49.552966Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Binarize Features","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import Binarizer\n\n# 3. Bernoulli Naive Bayes (binarize features)\nbinarizer = Binarizer()\nX_train_bin = binarizer.fit_transform(X_train)  # Fit only on training data\nX_test_bin = binarizer.transform(X_test)        # Transform the test data","metadata":{"execution":{"iopub.status.busy":"2024-10-25T04:59:20.180844Z","iopub.execute_input":"2024-10-25T04:59:20.181529Z","iopub.status.idle":"2024-10-25T04:59:20.199045Z","shell.execute_reply.started":"2024-10-25T04:59:20.181469Z","shell.execute_reply":"2024-10-25T04:59:20.197652Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Bernoulli Naive Bayes","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import BernoulliNB\n\n# 3. Bernoulli Naive Bayes\nbnb = BernoulliNB()\nbnb.fit(X_train_bin, y_train)\n\n# Make predictions with BernoulliNB\ny_pred_bnb = bnb.predict(X_test_bin)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T04:59:25.504963Z","iopub.execute_input":"2024-10-25T04:59:25.505986Z","iopub.status.idle":"2024-10-25T04:59:25.518578Z","shell.execute_reply.started":"2024-10-25T04:59:25.505927Z","shell.execute_reply":"2024-10-25T04:59:25.517185Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"# Print accuracies\nprint(\"GaussianNB Test Accuracy:\", gnb.score(X_test, y_test))\nprint(\"MultinomialNB Test Accuracy:\", mnb.score(X_test_scaled, y_test))\nprint(\"BernoulliNB Test Accuracy:\", bnb.score(X_test_bin, y_test))","metadata":{"execution":{"iopub.status.busy":"2024-10-25T04:59:52.215031Z","iopub.execute_input":"2024-10-25T04:59:52.215624Z","iopub.status.idle":"2024-10-25T04:59:52.232847Z","shell.execute_reply.started":"2024-10-25T04:59:52.215571Z","shell.execute_reply":"2024-10-25T04:59:52.231289Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### What is a Confusion Matrix?\n\nA **confusion matrix** is a table that helps us understand how well a machine learning model is doing when it comes to making predictions. It tells us how many predictions were correct and where the model made mistakes. It’s called \"confusion matrix\" because it shows where the model is confused in making predictions!\n\n#### Imagine this:\nLet’s say you are the teacher of a class, and you give a test to your students. After grading, you want to see how many students passed and how many failed. But you also want to check if your predictions about who would pass or fail were correct.\n\nSo you create a table to compare:\n- **Predicted** results (what you guessed).\n- **Actual** results (the real outcome).\n\nThe confusion matrix helps in comparing these two things: your **predictions** vs. the **truth**.\n\n### The Confusion Matrix Table\n\nHere’s what a confusion matrix looks like:\n\n|                    | **Predicted Positive** | **Predicted Negative** |\n|--------------------|------------------------|------------------------|\n| **Actual Positive** | True Positive (TP)     | False Negative (FN)    |\n| **Actual Negative** | False Positive (FP)    | True Negative (TN)     |\n\nNow, let’s break this down:\n\n1. **True Positive (TP):**\n   - These are the cases where the model correctly predicted **positive**, and the actual result is also **positive**.\n   - **Example:** If the model predicts that a student passed, and they actually passed, this is a true positive.\n\n2. **True Negative (TN):**\n   - These are the cases where the model correctly predicted **negative**, and the actual result is also **negative**.\n   - **Example:** If the model predicts that a student failed, and they actually failed, this is a true negative.\n\n3. **False Positive (FP):** (Also called a \"Type I Error\")\n   - These are the cases where the model predicted **positive**, but the actual result is **negative**.\n   - **Example:** If the model predicts that a student passed, but they actually failed, this is a false positive. This is a mistake!\n\n4. **False Negative (FN):** (Also called a \"Type II Error\")\n   - These are the cases where the model predicted **negative**, but the actual result is **positive**.\n   - **Example:** If the model predicts that a student failed, but they actually passed, this is a false negative. Another mistake!\n\n### Example in Real Life:\n\nLet’s say you’re a doctor, and you want to predict whether a patient has a disease. You perform a test and use a machine learning model to predict the result.\n\n- **Positive** = Patient has the disease.\n- **Negative** = Patient does not have the disease.\n\nNow the confusion matrix helps you evaluate how your model performed:\n\n|                    | **Predicted Disease**   | **Predicted No Disease** |\n|--------------------|-------------------------|--------------------------|\n| **Actual Disease**  | True Positive (TP)      | False Negative (FN)       |\n| **Actual No Disease**| False Positive (FP)     | True Negative (TN)        |\n\n- **True Positive (TP):** The model predicted that the patient has the disease, and the patient really has it.\n- **True Negative (TN):** The model predicted that the patient doesn’t have the disease, and the patient really doesn’t have it.\n- **False Positive (FP):** The model predicted that the patient has the disease, but the patient actually doesn’t (a false alarm!).\n- **False Negative (FN):** The model predicted that the patient doesn’t have the disease, but the patient actually does (this could be dangerous!).\n\n### What Does the Confusion Matrix Tell Us?\n\nOnce we fill the confusion matrix with numbers, we can use it to calculate important metrics to evaluate how good the model is:\n\n1. **Accuracy:**\n   - How often the model made the correct prediction.\n   $$\n   Accuracy = \\frac{TP + TN}{TP + TN + FP + FN} \n   $$\n\n   - Accuracy tells us the proportion of correct predictions, but it can be misleading if there’s an imbalance between the classes.\n\n2. **Precision:**\n   - Out of all the times the model predicted **positive**, how many were actually positive?\n   $$\n   Precision = \\frac{TP}{TP + FP}\n   $$\n   \n   - Precision helps us know how much we can trust the positive predictions.\n\n3. **Recall (Sensitivity):**\n   - Out of all the actual **positive** cases, how many did the model correctly identify?\n   $$\n   Recall = \\frac{TP}{TP + FN}\n   $$\n   - Recall tells us how well the model is at identifying positive cases.\n\n4. **F1-Score:**\n   - The F1-Score combines precision and recall into one number, especially useful when we want a balance between them.\n   $$\n   F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}\n   $$\n   \n   - It gives a more balanced view when we want both precision and recall to be considered\n\n\n### Example for a Clear Understanding:\n\nLet’s imagine you’re testing a model that predicts whether students pass an exam based on their practice scores. You predicted the results for 10 students.\n\n- **Actual Results:** 6 students passed (Positive), 4 students failed (Negative).\n- **Predictions:**\n   - 5 students were predicted to pass (3 of them really passed, 2 didn’t).\n   - 5 students were predicted to fail (3 of them really failed, 2 didn’t).\n\nNow the confusion matrix would look like this:\n\n|                    | **Predicted Pass**      | **Predicted Fail**      |\n|--------------------|-------------------------|-------------------------|\n| **Actual Pass**     | 3 (TP)                  | 3 (FN)                  |\n| **Actual Fail**     | 2 (FP)                  | 2 (TN)                  |\n\n- **True Positive (TP)** = 3 (The model correctly predicted 3 students passed).\n- **True Negative (TN)** = 2 (The model correctly predicted 2 students failed).\n- **False Positive (FP)** = 2 (The model incorrectly predicted 2 students would pass, but they failed).\n- **False Negative (FN)** = 3 (The model incorrectly predicted 3 students would fail, but they passed).\n\nNow you can calculate accuracy, precision, recall, etc., based on this matrix!\n\n### In Summary:\n- The confusion matrix is a simple table showing **correct** and **incorrect** predictions.\n- It helps us calculate **accuracy, precision, recall, and F1-score**, which tell us how well our model is performing.\n- **True Positive (TP), True Negative (TN), False Positive (FP), and False Negative (FN)** help us understand where the model gets it right or wrong.\n\nUnderstanding the confusion matrix helps you evaluate the model’s strengths and weaknesses clearly!\n\n","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report\n\nprint(\"=== === === === === === === === === === Gaussian Naive Bayes === === === === === === === === === ===\")\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred_gnb))\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred_gnb))\n# -------------------------------------------------------\nprint(\"\\n=== === === === === === === === === === Multinomial Naive Bayes === === === === === === === === === ===\")\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred_mnb))\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred_mnb))\n# -------------------------------------------------------\nprint(\"\\n=== === === === === === === === === === Bernoulli Naive Bayes === === === === === === === === === ===\")\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred_bnb))\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred_bnb))\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T05:08:02.154957Z","iopub.execute_input":"2024-10-25T05:08:02.156029Z","iopub.status.idle":"2024-10-25T05:08:02.205668Z","shell.execute_reply.started":"2024-10-25T05:08:02.155961Z","shell.execute_reply":"2024-10-25T05:08:02.204386Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Insights\nIn your classification report, the values **0** and **1** typically represent the different classes of the target variable you are predicting. Based on the context of your dataset (which seems to be a gender classification dataset), here’s what they likely indicate:\n\n- **0**: Represents one gender (e.g., Male)\n- **1**: Represents the other gender (e.g., Female)\n\n### Breakdown of the Classification Report:\n- **Precision**: This metric indicates the proportion of positive identifications (correct predictions) that were actually correct. \n  - For **class 0** (precision = 0.96): Out of all instances predicted as class 0, 96% were correctly identified.\n  - For **class 1** (precision = 0.97): Out of all instances predicted as class 1, 97% were correctly identified.\n\n- **Recall**: This metric indicates the proportion of actual positives that were correctly identified.\n  - For **class 0** (recall = 0.97): Out of all actual instances of class 0, 97% were correctly predicted.\n  - For **class 1** (recall = 0.96): Out of all actual instances of class 1, 96% were correctly predicted.\n\n- **F1-Score**: This is the harmonic mean of precision and recall. It provides a balance between the two metrics, especially useful when you have an uneven class distribution.\n  - Both classes have an F1-score of around 0.96, indicating good performance.\n\n- **Support**: This indicates the number of actual occurrences of the class in the specified dataset.\n  - For class 0: 502 instances\n  - For class 1: 499 instances\n\n- **Overall Accuracy**: The accuracy of the model across all instances is **96%**.\n\n### Interpretation\nIn summary, the model performs very well on both classes, with high precision, recall, and F1-scores close to 0.96 for both genders. This suggests that the classifier is effective in distinguishing between the two genders based on the features provided. \n\nIf you have further questions or need clarification on specific points, feel free to ask!","metadata":{}},{"cell_type":"markdown","source":"# Confusion Matrix & Classification Report\n\n1. **Confusion Matrix:**\n   - The `confusion_matrix` function compares the true values (`y_test`) with the predicted values (`y_pred_*` for each model).\n   - It returns a matrix that helps you understand how many predictions were correct and where the model made errors (True Positives, True Negatives, False Positives, and False Negatives).\n\n2. **Classification Report:**\n   - The `classification_report` gives you detailed metrics:\n     - **Precision:** How many selected items are relevant.\n     - **Recall:** How many relevant items are selected.\n     - **F1-Score:** The balance between precision and recall.\n     - **Support:** The number of actual occurrences of each class in the dataset.\n\n### Example Output for Confusion Matrix:\n\nFor a binary classification problem, the confusion matrix may look like this:\n\n```\n[[TN  FP]\n [FN  TP]]\n```\n\n- **TN (True Negative):** Correctly predicted negative cases.\n- **FP (False Positive):** Incorrectly predicted positive cases (Type I error).\n- **FN (False Negative):** Incorrectly predicted negative cases (Type II error).\n- **TP (True Positive):** Correctly predicted positive cases.\n\nBy examining this matrix and the classification report, you'll have a clear picture of how well each Naive Bayes model is performing.","metadata":{}},{"cell_type":"markdown","source":"# Correlation heatmap\nTo create a **correlation heatmap** and extract insights from the confusion matrix, we can visualize how the true labels and predicted labels correlate. This can give you a better understanding of the model's performance, particularly in terms of misclassifications.\n\n### Steps:\n1. **Calculate the confusion matrix.**\n2. **Convert the confusion matrix into a DataFrame** for easier manipulation.\n3. **Create a heatmap** using `seaborn` or `matplotlib` to visualize the confusion matrix.\n4. **Interpret insights** from the heatmap: Focus on where the model is performing well (high correlations on the diagonal) and where it's struggling (non-diagonal elements).\n\nHere’s how you can do it for one model, e.g., Gaussian Naive Bayes:","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n\n# Step 1: Calculate the confusion matrix for Gaussian Naive Bayes\ncm_gnb = confusion_matrix(y_test, y_pred_gnb)\n\n# Step 3: Convert confusion matrix into a DataFrame for better visualization\ncm_df_gnb = pd.DataFrame(cm_gnb, index=[\"Actual Negative\", \"Actual Positive\"], \n                     columns=[\"Predicted Negative\", \"Predicted Positive\"])\n\n\n# Step 3: Plot a heatmap of the confusion matrix\nplt.figure(figsize=(6, 4))\nsns.heatmap(cm_df_gnb, annot=True, cmap=\"Blues\", fmt=\"g\", cbar=False)\nplt.title(\"Gaussian Naive Bayes Confusion Matrix Heatmap\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-25T05:11:30.811256Z","iopub.execute_input":"2024-10-25T05:11:30.812657Z","iopub.status.idle":"2024-10-25T05:11:31.466948Z","shell.execute_reply.started":"2024-10-25T05:11:30.812591Z","shell.execute_reply":"2024-10-25T05:11:31.465595Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate and display metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\n\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_gnb))\nprint(\"Precision:\", precision_score(y_test, y_pred_gnb))\nprint(\"Recall:\", recall_score(y_test, y_pred_gnb))\nprint(\"F1 Score:\", f1_score(y_test, y_pred_gnb))","metadata":{"execution":{"iopub.status.busy":"2024-10-25T05:14:51.855336Z","iopub.execute_input":"2024-10-25T05:14:51.855836Z","iopub.status.idle":"2024-10-25T05:14:51.876948Z","shell.execute_reply.started":"2024-10-25T05:14:51.85579Z","shell.execute_reply":"2024-10-25T05:14:51.875699Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Step 1: Calculate the confusion matrix for Gaussian Naive Bayes\ncm_mnb = confusion_matrix(y_test, y_pred_mnb)\n\n# Step 3: Convert confusion matrix into a DataFrame for better visualization\ncm_df_mnb = pd.DataFrame(cm_mnb, index=[\"Actual Negative\", \"Actual Positive\"], \n                     columns=[\"Predicted Negative\", \"Predicted Positive\"])\n\n\n# Step 3: Plot a heatmap of the confusion matrix\nplt.figure(figsize=(6, 4))\nsns.heatmap(cm_df_mnb, annot=True, cmap=\"Blues\", fmt=\"g\", cbar=False)\nplt.title(\"Multinomial Naive Bayes Confusion Matrix Heatmap\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-25T05:15:07.514133Z","iopub.execute_input":"2024-10-25T05:15:07.515884Z","iopub.status.idle":"2024-10-25T05:15:07.723762Z","shell.execute_reply.started":"2024-10-25T05:15:07.515806Z","shell.execute_reply":"2024-10-25T05:15:07.722382Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate and display metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\n\n# Calculate and display metrics\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_mnb))\nprint(\"Precision:\", precision_score(y_test, y_pred_mnb))\nprint(\"Recall:\", recall_score(y_test, y_pred_mnb))\nprint(\"F1 Score:\", f1_score(y_test, y_pred_mnb))","metadata":{"execution":{"iopub.status.busy":"2024-10-25T05:15:24.546541Z","iopub.execute_input":"2024-10-25T05:15:24.54806Z","iopub.status.idle":"2024-10-25T05:15:24.570862Z","shell.execute_reply.started":"2024-10-25T05:15:24.547993Z","shell.execute_reply":"2024-10-25T05:15:24.569404Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Step 1: Calculate the confusion matrix for Gaussian Naive Bayes\ncm_bnb = confusion_matrix(y_test, y_pred_bnb)\n\n# Step 3: Convert confusion matrix into a DataFrame for better visualization\ncm_df_bnb = pd.DataFrame(cm_bnb, index=[\"Actual Negative\", \"Actual Positive\"], \n                     columns=[\"Predicted Negative\", \"Predicted Positive\"])\n\n\n# Step 3: Plot a heatmap of the confusion matrix\nplt.figure(figsize=(6, 4))\nsns.heatmap(cm_df_bnb, annot=True, cmap=\"Blues\", fmt=\"g\", cbar=False)\nplt.title(\"Bernoulli Naive Bayes Confusion Matrix Heatmap\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-25T05:15:40.186686Z","iopub.execute_input":"2024-10-25T05:15:40.188107Z","iopub.status.idle":"2024-10-25T05:15:40.402826Z","shell.execute_reply.started":"2024-10-25T05:15:40.188042Z","shell.execute_reply":"2024-10-25T05:15:40.401516Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate and display metrics\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_bnb))\nprint(\"Precision:\", precision_score(y_test, y_pred_bnb))\nprint(\"Recall:\", recall_score(y_test, y_pred_bnb))\nprint(\"F1 Score:\", f1_score(y_test, y_pred_bnb))","metadata":{"execution":{"iopub.status.busy":"2024-10-25T05:15:45.075119Z","iopub.execute_input":"2024-10-25T05:15:45.075653Z","iopub.status.idle":"2024-10-25T05:15:45.094633Z","shell.execute_reply.started":"2024-10-25T05:15:45.075604Z","shell.execute_reply":"2024-10-25T05:15:45.093361Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Insights from the Correlation Chart (Confusion Matrix Heatmap):\n\n- **Diagonal Elements (True Positives and True Negatives):**\n  - High values on the diagonal (top left and bottom right) represent correct predictions:\n    - **Top Left (True Negatives - TN):** These are the cases where the model correctly predicted \"negative.\"\n    - **Bottom Right (True Positives - TP):** These are the cases where the model correctly predicted \"positive.\"\n  - A high number on these diagonal elements indicates that the model is making many correct predictions.\n\n- **Off-Diagonal Elements (False Positives and False Negatives):**\n  - Non-diagonal values (top right and bottom left) represent errors:\n    - **Top Right (False Positives - FP):** The model incorrectly predicted positive when it was actually negative. A high value here might indicate overprediction of the positive class.\n    - **Bottom Left (False Negatives - FN):** The model incorrectly predicted negative when it was actually positive. A high value here means the model is missing positive cases, which could be concerning in critical applications (like detecting diseases).\n  - A high value on the off-diagonal elements suggests that the model is making many errors.\n\n### How to Interpret:\n- **Balanced Performance:** If the diagonal values are significantly higher than the off-diagonal values, your model is performing well.\n- **Misclassifications:** If the off-diagonal values are high, focus on where the model is making mistakes (False Positives or False Negatives). This may help you adjust thresholds, collect more data, or choose a better-suited model.","metadata":{}},{"cell_type":"markdown","source":"---\n# Accuracy\nSimply looking at **accuracy** alone is not sufficient to determine whether a model is suitable or reliable for your problem. Accuracy can be misleading in several cases, especially when dealing with **imbalanced datasets** or when your model might be overfitting or underfitting. Here's why:\n\n### 1. **Imbalanced Datasets:**\n   - If your dataset has a significant class imbalance (e.g., 95% of one class and 5% of another), accuracy might give a false sense of high performance. For example, if your model predicts the majority class all the time, it will have high accuracy, but it will fail to identify the minority class, which could be more important.\n   - **Example:** In a dataset where 95% of the data belongs to Class A and only 5% to Class B, if the model predicts Class A for all instances, the accuracy will be 95%, but the model will be useless in detecting Class B.\n\n   **What to use instead:** Metrics like **Precision, Recall, F1-Score**, or **ROC-AUC** (for binary classification) are better indicators for imbalanced data.\n\n### 2. **Overfitting:**\n   - A model might show very high accuracy on the training data but perform poorly on unseen data (test data). This is known as **overfitting**, where the model has learned the noise in the training set rather than the actual patterns.\n   - Overfitting can be identified when the model has significantly higher accuracy on the training set than on the test set.\n\n   **What to use instead:** Look at the difference in accuracy (or other metrics) between training and test sets. Using **cross-validation** can also help to assess model stability across different subsets of data.\n\n### 3. **Underfitting:**\n   - If the accuracy is low on both the training and test sets, it could indicate **underfitting**, where the model is too simple to capture the underlying patterns in the data.\n   \n   **What to use instead:** Consider if your model is too basic and might need more features, a more complex algorithm, or better tuning of hyperparameters.\n\n### 4. **Accuracy Doesn't Account for Misclassifications:**\n   - Accuracy only tells you how many instances were correctly classified out of the total. It doesn't provide details on **what types of errors** the model is making. For example, predicting a disease in medical diagnostics might require minimizing **false negatives** (cases where the disease is predicted to be absent when it is actually present), and accuracy might not be the best metric to capture this.\n\n   **What to use instead:** Evaluate performance using a **Confusion Matrix**, where you can examine false positives and false negatives. From there, metrics like **Precision**, **Recall**, and **F1-Score** can help you decide whether the model is suitable for your use case.\n\n---\n\n### Key Points:\n- **Accuracy is just one metric** and might be misleading in certain scenarios, especially with imbalanced datasets.\n- **Additional metrics** like **Precision, Recall, F1-Score, and AUC-ROC** provide more insight into the model's actual performance.\n- Use **cross-validation** and compare performance across the training and test sets to check for **overfitting or underfitting**.\n\nBy evaluating these aspects, you can better understand whether a model is suitable for your problem beyond just its accuracy score.\n---","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"code","source":"# next knn","metadata":{"execution":{"iopub.status.busy":"2024-10-23T08:48:11.537623Z","iopub.execute_input":"2024-10-23T08:48:11.537979Z","iopub.status.idle":"2024-10-23T08:48:11.542566Z","shell.execute_reply.started":"2024-10-23T08:48:11.537941Z","shell.execute_reply":"2024-10-23T08:48:11.541544Z"},"trusted":true},"outputs":[],"execution_count":null}]}